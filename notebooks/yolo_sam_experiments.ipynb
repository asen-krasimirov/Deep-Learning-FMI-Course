# 1. Setup Kaggle Environment and Install Libraries
# --------------------------------------------------

# Install ultralytics (for YOLOv8 detection)
!pip install -q ultralytics

# Install Segment Anything
!pip install 'git+https://github.com/facebookresearch/segment-anything.git'

# Download SAM model weights (ViT-H)
!wget -q 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth'

# Clear output for cleaner notebook
from IPython.display import clear_output
clear_output()

print("Environment setup complete!")

# 2. Import Necessary Libraries
# -----------------------------
import torch
import cv2
import numpy as np
import os
from tqdm.notebook import tqdm # For progress bars
from PIL import Image
from glob import glob
import shutil # For copying files

# SAM imports
from segment_anything import sam_model_registry, SamPredictor # Use SamPredictor for bounding box prompts

# YOLOv8 imports
from ultralytics import YOLO

# 3. Define Constants and Paths
# -----------------------------
DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {DEVICE}")

SAM_CHECKPOINT_PATH = 'sam_vit_h_4b8939.pth'
SAM_MODEL_TYPE = "vit_h"

FOOD101_PATH = '/kaggle/input/food41/food41/' # Adjust if your Food-101 path is different
images_dir = os.path.join(FOOD101_PATH, 'images')
meta_dir = os.path.join(FOOD101_PATH, 'meta') # Contains classes.txt, train.txt, test.txt

# Path to your trained YOLOv8 Object Detection model for Food-101
# You need to have trained this model previously on Food-101 for object detection.
# Example: If you trained it on Food-101 and saved the best weights:
# If you uploaded it as a Kaggle dataset:
YOLOV8_DETECTION_MODEL_PATH = '/kaggle/input/your-yolov8-food-detection-dataset/best.pt' # ADJUST THIS
# If you just want to test with a general YOLOv8 model (less accurate for food types):
# YOLOV8_DETECTION_MODEL_PATH = 'yolov8s.pt' # Downloaded by ultralytics automatically

OUTPUT_LABELS_DIR = 'yolo_sam_autogenerated_seg_labels'
os.makedirs(os.path.join(OUTPUT_LABELS_DIR, 'images'), exist_ok=True)
os.makedirs(os.path.join(OUTPUT_LABELS_DIR, 'labels'), exist_ok=True)


# 4. Load Food-101 Categories and Create Mappings
# ------------------------------------------------
classes_file = os.path.join(meta_dir, 'classes.txt')
if os.path.exists(classes_file):
    with open(classes_file, 'r') as f:
        categories = [line.strip() for line in f.readlines()]
else:
    categories = sorted(os.listdir(images_dir))
    categories = [d for d in categories if os.path.isdir(os.path.join(images_dir, d))]

num_classes_food101 = len(categories)
category_to_id = {name: i for i, name in enumerate(categories)}
id_to_category = {i: name for i, name in enumerate(categories)}

print(f"Loaded {num_classes_food101} food categories.")

# 5. Load YOLOv8 Object Detection Model
# -------------------------------------
model_yolo_detector = YOLO(YOLOV8_DETECTION_MODEL_PATH)
model_yolo_detector.to(DEVICE)
print(f"YOLOv8 Detection model loaded from {YOLOV8_DETECTION_MODEL_PATH}")

# 6. Initialize SAM Predictor
# ---------------------------
sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT_PATH)
sam.to(device=DEVICE)
sam_predictor = SamPredictor(sam) # Use SamPredictor for specific prompts (bounding boxes)
print("SAM Predictor initialized.")

# Helper function to convert mask to polygon points for YOLOv8 segmentation format
def mask_to_yolov8_polygon(mask, image_width, image_height):
    """Converts a boolean mask to normalized polygon coordinates."""
    # Find contours from the mask
    # `np.ascontiguousarray` is important for cv2.findContours
    binary_mask_uint8 = np.ascontiguousarray(mask.astype(np.uint8) * 255)
    contours, _ = cv2.findContours(binary_mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    if not contours:
        return []

    # Take the largest contour (assuming it's the primary object)
    largest_contour = max(contours, key=cv2.contourArea)

    # Flatten and normalize polygon points
    polygon_points = largest_contour.flatten().tolist()
    
    normalized_polygon = []
    for i in range(0, len(polygon_points), 2):
        x_coord, y_coord = polygon_points[i], polygon_points[i+1]
        normalized_polygon.append(f"{x_coord / image_width:.6f}")
        normalized_polygon.append(f"{y_coord / image_height:.6f}")
